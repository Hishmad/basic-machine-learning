{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Lesson Eight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### TensorBoard\n",
    "According to the documentations:\n",
    "The computations you'll use TensorFlow for - like training a massive deep neural network - can be complex and confusing. To make it easier to understand, debug, and optimize TensorFlow programs, we've included a suite of visualization tools called TensorBoard. You can use TensorBoard to visualize your TensorFlow graph, plot quantitative metrics about the execution of your graph, and show additional data like images that pass through it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Let's import from lesson_six\n",
    "import lesson_six as lesson_six\n",
    "# Rest the graph\n",
    "lesson_six.reset_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "# Instanciat the data\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "# The shape of the data is (20640, 8) this will return m is the number of rows, n is the number of column\n",
    "m, n = housing.data.shape \n",
    "#housing_data_and_bias = np.c_[np.ones((m, 1)), housing.data]\n",
    "\n",
    "# We have to standardiaze the input data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Instantiate the preprocessing object\n",
    "scaler = StandardScaler() \n",
    "\n",
    "# This will standardize the input data, the shape will be (m, 8)\n",
    "scaled_housing_data = scaler.fit_transform(housing.data) \n",
    "\n",
    "# This will add bias, this will add column at index 0, the shape will be (m, 8+1) \n",
    "scaled_housing_data_and_bias = np.c_[np.ones((m, 1)), scaled_housing_data] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def fetch_batch(epoch, batch_index, batch_size):\n",
    "    '''\n",
    "    This fucntion will return the X_batch, y_batch with random indices\n",
    "    '''\n",
    "    \n",
    "    # Find a random indices whithin the size of the dataset\n",
    "    np.random.seed(epoch * n_batches + batch_index)\n",
    "    indices = np.random.randint(m, size=batch_size)\n",
    "    \n",
    "    # Pich the X and y batches using the random indices\n",
    "    X_batch = scaled_housing_data_and_bias[indices]\n",
    "    y_batch = housing.target.reshape(-1, 1)[indices]\n",
    "    return X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# First we will create a log\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def getLogDir():\n",
    "    now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "    root_logdir = \"/Volumes/MacAndroidStudio/ml-books/savedModel/tf_logs\"\n",
    "    logdir = \"{}/run-{}/\".format(root_logdir, now)\n",
    "    \n",
    "    return logdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Change the definition of X and y in the construction phase to make them a placeholder nodes.\n",
    "X = tf.placeholder(tf.float32, shape=(None, n + 1), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "\n",
    "# Compute the theta with tf.random_uniform() use the number of column add 1 so it will match the X+bias columns\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "\n",
    "# Compute the y_hat using tf.matmul()\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "\n",
    "# Get the error\n",
    "error = y_pred - y\n",
    "\n",
    "# Compute the Mean Square Error using tf.reduce_mean() and tf.square(error)\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "\n",
    "# Gradient Decscent with optimizer\n",
    "#optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.5)\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "# Globa variable initializer\n",
    "global_init = tf.global_variables_initializer()\n",
    "\n",
    "# This will create a node in the graph that will evaluate the MSE value and write it to a TensorBoard\n",
    "# binary log string called a summary.\n",
    "mse_summary = tf.summary.scalar('MSE', mse)\n",
    "\n",
    "# This will create FileWriter object that we will use to write summaries to logfiles in the log directory\n",
    "# The first parameter is the log directory path, and the second parameter which optional is the graph to visualize.\n",
    "file_writer = tf.summary.FileWriter(getLogDir(), tf.get_default_graph())\n",
    "\n",
    "# Define the batch size and compute the total number of batches:\n",
    "batch_size = 100\n",
    "n_batches = int(np.ceil(m / batch_size))\n",
    "\n",
    "# The running phase...\n",
    "with tf.Session() as sess:\n",
    "    # Pass the tf.global_variables_initializer() to the Session.run()\n",
    "    sess.run(global_init)                                                                \n",
    "\n",
    "    # iterate over\n",
    "    for epoch in range(n_epochs): \n",
    "        \n",
    "        for batch_index in range(n_batches):\n",
    "            \n",
    "            # Fetching the mini-batches one by one\n",
    "            X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size)\n",
    "            \n",
    "            # For every 10 mini-batches this code will evaluate the mse_summary node during training.\n",
    "            # Then write the result to the file log using the FileWriter object\n",
    "            if batch_index % 10 == 0:\n",
    "                summary_str = mse_summary.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                step = epoch * n_batches + batch_index\n",
    "                file_writer.add_summary(summary_str, step)\n",
    "            \n",
    "            # Invoke run() and pass the training optimizer, with the X and y batches\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "\n",
    "    # Pass the result here\n",
    "    best_theta = theta.eval() \n",
    "\n",
    "# Close the FileWriter to free resources.\n",
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.049927  ],\n",
       "       [ 0.8267504 ],\n",
       "       [ 0.11433333],\n",
       "       [-0.23890038],\n",
       "       [ 0.31248516],\n",
       "       [ 0.03146332],\n",
       "       [-1.4988426 ],\n",
       "       [-0.87141562],\n",
       "       [-0.83012849]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "Fromt the terminal type this:\n",
    "$ tensorboard --logdir=path/to/log-directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back to basic again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lesson_six.reset_graph()\n",
    "\n",
    "# Create a global variable\n",
    "W = tf.Variable([.3], dtype=tf.float32)\n",
    "b = tf.Variable([-.3], dtype=tf.float32)\n",
    "\n",
    "# Create a placeholder: it is a promise to provide a value later.\n",
    "x = tf.placeholder(tf.float32)\n",
    "\n",
    "# This is a linear operation\n",
    "linear_model = W * x + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This will initialize all the global variables, in this case W and b\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Instantiate the Session()\n",
    "sess = tf.Session()\n",
    "\n",
    "# Pass the init to the run() \n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.          0.30000001  0.60000002  0.90000004]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Here where all the computation is done:\n",
    "# linear_mobel has a reference to W, x and b, in which its already know the value of W and b but not yet x\n",
    "# because x is a placeholder and it is a promise to provide value later\n",
    "# So this will result in processing every element in x, in according to linear_model.\n",
    "print(sess.run(linear_model, {x:[1,2,3,4]}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.66\n"
     ]
    }
   ],
   "source": [
    "# y is the target value, or often called the label\n",
    "y = tf.placeholder(tf.float32)\n",
    "\n",
    "# The tf.square() will square the error\n",
    "squared_deltas = tf.square(linear_model - y)\n",
    "\n",
    "# The tf.reduce_sum() will get the loss of the squared error\n",
    "loss = tf.reduce_sum(squared_deltas)\n",
    "\n",
    "# Now lets evaluate and pass the data for x and y\n",
    "print(sess.run(loss, {x:[1,2,3,4], y:[0,-1,-2,-3]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-1.], dtype=float32), array([ 1.], dtype=float32)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The tf.assign is to change/modify the value of a variable\n",
    "fixW = tf.assign(W, [-1.])\n",
    "fixb = tf.assign(b, [1.])\n",
    "\n",
    "# this is same as initializeing\n",
    "sess.run([fixW, fixb])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# This will evaluate the loss, given the value of x and y\n",
    "print(sess.run(loss, {x:[1,2,3,4], y:[0,-1,-2,-3]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consequently, TensorFlow can automatically produce derivatives given only a description of the model using the function tf.gradients. For simplicity, optimizers typically do this for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We pass the learning rate as argument\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "\n",
    "# then we pass the loss to the optimizer.minimize()\n",
    "train = optimizer.minimize(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess.run(init) # reset values to incorrect defaults.\n",
    "for i in range(1000):\n",
    "    sess.run(train, {x:[1,2,3,4], y:[0,-1,-2,-3]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W: [-0.9999969] b: [ 0.99999082] loss: 5.69997e-11\n"
     ]
    }
   ],
   "source": [
    "# evaluate training accuracy\n",
    "curr_W, curr_b, curr_loss = sess.run([W, b, loss], {x:[1,2,3,4], y:[0,-1,-2,-3]})\n",
    "print(\"W: %s b: %s loss: %s\"%(curr_W, curr_b, curr_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From the documentations: \n",
    "\n",
    "tf.contrib.learn is a high-level TensorFlow library that simplifies the mechanics of machine learning, including the following:\n",
    "\n",
    "1. running training loops\n",
    "2. running evaluation loops\n",
    "3. managing data sets\n",
    "4. managing feeding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lesson_six.reset_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/v8/qd7dlmk9103cms8fxm9t5_600000gn/T/tmpemi38j0o\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x11a4c30b8>, '_master': '', '_num_ps_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1\n",
      "}\n",
      ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000}\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:From /Users/hishmadabubakaralamudi/anaconda3/envs/dlnd/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/head.py:1362: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /var/folders/v8/qd7dlmk9103cms8fxm9t5_600000gn/T/tmpemi38j0o/model.ckpt.\n",
      "INFO:tensorflow:loss = 3.5, step = 1\n",
      "INFO:tensorflow:global_step/sec: 863.813\n",
      "INFO:tensorflow:loss = 0.0433291, step = 101\n",
      "INFO:tensorflow:global_step/sec: 282.968\n",
      "INFO:tensorflow:loss = 0.0250619, step = 201\n",
      "INFO:tensorflow:global_step/sec: 966.211\n",
      "INFO:tensorflow:loss = 0.00444322, step = 301\n",
      "INFO:tensorflow:global_step/sec: 1098.03\n",
      "INFO:tensorflow:loss = 0.000717889, step = 401\n",
      "INFO:tensorflow:global_step/sec: 412.584\n",
      "INFO:tensorflow:loss = 7.35314e-05, step = 501\n",
      "INFO:tensorflow:global_step/sec: 939.399\n",
      "INFO:tensorflow:loss = 6.49574e-06, step = 601\n",
      "INFO:tensorflow:global_step/sec: 887.053\n",
      "INFO:tensorflow:loss = 4.00174e-06, step = 701\n",
      "INFO:tensorflow:global_step/sec: 1021.2\n",
      "INFO:tensorflow:loss = 3.97147e-07, step = 801\n",
      "INFO:tensorflow:global_step/sec: 1246.99\n",
      "INFO:tensorflow:loss = 4.45591e-07, step = 901\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into /var/folders/v8/qd7dlmk9103cms8fxm9t5_600000gn/T/tmpemi38j0o/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 6.91743e-08.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:From /Users/hishmadabubakaralamudi/anaconda3/envs/dlnd/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/head.py:1362: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n",
      "INFO:tensorflow:Starting evaluation at 2017-07-02-14:22:25\n",
      "INFO:tensorflow:Finished evaluation at 2017-07-02-14:22:26\n",
      "INFO:tensorflow:Saving dict for global step 1000: global_step = 1000, loss = 5.24289e-08\n",
      "WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:From /Users/hishmadabubakaralamudi/anaconda3/envs/dlnd/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/head.py:1362: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n",
      "INFO:tensorflow:Starting evaluation at 2017-07-02-14:22:26\n",
      "INFO:tensorflow:Finished evaluation at 2017-07-02-14:22:27\n",
      "INFO:tensorflow:Saving dict for global step 1000: global_step = 1000, loss = 0.00254591\n",
      "WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.\n",
      "train loss: {'loss': 5.2428909e-08, 'global_step': 1000}\n",
      "eval loss: {'loss': 0.0025459128, 'global_step': 1000}\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# NumPy is often used to load, manipulate and preprocess data.\n",
    "import numpy as np\n",
    "\n",
    "# Declare list of features. We only have one real-valued feature. There are many\n",
    "# other types of columns that are more complicated and useful.\n",
    "features = [tf.contrib.layers.real_valued_column(\"x\", dimension=1)]\n",
    "\n",
    "# An estimator is the front end to invoke training (fitting) and evaluation\n",
    "# (inference). There are many predefined types like linear regression,\n",
    "# logistic regression, linear classification, logistic classification, and\n",
    "# many neural network classifiers and regressors. The following code\n",
    "# provides an estimator that does linear regression.\n",
    "estimator = tf.contrib.learn.LinearRegressor(feature_columns=features)\n",
    "\n",
    "# TensorFlow provides many helper methods to read and set up data sets.\n",
    "# Here we use two data sets: one for training and one for evaluation\n",
    "# We have to tell the function how many batches\n",
    "# of data (num_epochs) we want and how big each batch should be.\n",
    "x_train = np.array([1., 2., 3., 4.])\n",
    "y_train = np.array([0., -1., -2., -3.])\n",
    "x_eval = np.array([2., 5., 8., 1.])\n",
    "y_eval = np.array([-1.01, -4.1, -7, 0.])\n",
    "input_fn = tf.contrib.learn.io.numpy_input_fn({\"x\":x_train}, y_train,\n",
    "                                              batch_size=4,\n",
    "                                              num_epochs=1000)\n",
    "eval_input_fn = tf.contrib.learn.io.numpy_input_fn(\n",
    "    {\"x\":x_eval}, y_eval, batch_size=4, num_epochs=1000)\n",
    "\n",
    "# We can invoke 1000 training steps by invoking the  method and passing the\n",
    "# training data set.\n",
    "estimator.fit(input_fn=input_fn, steps=1000)\n",
    "\n",
    "# Here we evaluate how well our model did.\n",
    "train_loss = estimator.evaluate(input_fn=input_fn)\n",
    "eval_loss = estimator.evaluate(input_fn=eval_input_fn)\n",
    "print(\"train loss: %r\"% train_loss)\n",
    "print(\"eval loss: %r\"% eval_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
