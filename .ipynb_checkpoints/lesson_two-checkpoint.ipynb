{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Lesson Two"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "In lesson one, we used a decision tree as our classifier. In this lesson we'll add code to visualize it so we can see how it works under the hood. There are many types of classifiers:\n",
    "1. Artificial Neural Network\n",
    "2. Support Vector Machine\n",
    "3. Decision Tree\n",
    "and More…"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So why did we use a decision tree to start? Well, they have a very unique property--they're easy to read and understand. In fact, they're one of the few models that are interpretable, where you can understand exactly why the classifier makes a decision. That's amazingly useful in practice. To get started, I'll introduce you to a real data set we'll work with today. It's called Iris. Iris is a classic machine learning problem. In it, you want to identify what type of flower you have based on different measurements, like the length and width of the petal. The data set includes three different types of flowers. They're all species of iris-- setosa, versicolor, and virginica.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrolling down, you can see we’re given 50 examples of each type, so 150 examples total. Notice there are four features that are used to describe each example. These are the length and width of the sepal and petal. And just like in our apples and oranges problem, the first four columns give the features and the last column gives the labels, which is the type of flower in each row. Our goal is to use this data set to train a classifier. Then we can use that classifier to predict what species of flower we have if we're given a new flower that we've never seen before. Knowing how to work with an existing data set is a good skill, so let's import Iris into Scikit-learn and see what it looks like in code. Conveniently, the friendly folks at Scikit-learn provided a bunch of sample data sets, including Iris, as well as utilities to make them easy to import."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import dataset\n",
    "from sklearn.datasets import load_iris\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can import Iris into our code like this. The data set includes both the table from Wikipedia as well as some metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "['setosa' 'versicolor' 'virginica']\n",
      "[ 5.1  3.5  1.4  0.2]\n"
     ]
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "print(iris.feature_names)\n",
    "print(iris.target_names)\n",
    "print(iris.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The metadata tells you the names of the features and the names of different types of flowers. The features and examples themselves are contained in the data variable. For example, if I print out the first entry, you can see the measurements for this flower. These index to the feature names, so the first value refers to the sepal length, and the second to sepal width, and so on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(iris.target[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target variable contains the labels. Likewise, these index to the target names. Let's print out the first one. A label of 0 means it's a setosa. If you look at the table from Wikipedia, you'll notice that we just printed out the first row. Now both the data and target variables have 150 entries. If you want, you can iterate over them to print out the entire data set like this. Now that we know how to work with the data set, we're ready to train a classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example:  0  label:  0  features:  [ 5.1  3.5  1.4  0.2]\n",
      "Example:  1  label:  0  features:  [ 4.9  3.   1.4  0.2]\n",
      "Example:  2  label:  0  features:  [ 4.7  3.2  1.3  0.2]\n",
      "Example:  3  label:  0  features:  [ 4.6  3.1  1.5  0.2]\n",
      "Example:  4  label:  0  features:  [ 5.   3.6  1.4  0.2]\n",
      "Example:  5  label:  0  features:  [ 5.4  3.9  1.7  0.4]\n",
      "Example:  6  label:  0  features:  [ 4.6  3.4  1.4  0.3]\n",
      "Example:  7  label:  0  features:  [ 5.   3.4  1.5  0.2]\n",
      "Example:  8  label:  0  features:  [ 4.4  2.9  1.4  0.2]\n",
      "Example:  9  label:  0  features:  [ 4.9  3.1  1.5  0.1]\n",
      "Example:  10  label:  0  features:  [ 5.4  3.7  1.5  0.2]\n",
      "Example:  11  label:  0  features:  [ 4.8  3.4  1.6  0.2]\n",
      "Example:  12  label:  0  features:  [ 4.8  3.   1.4  0.1]\n",
      "Example:  13  label:  0  features:  [ 4.3  3.   1.1  0.1]\n",
      "Example:  14  label:  0  features:  [ 5.8  4.   1.2  0.2]\n",
      "Example:  15  label:  0  features:  [ 5.7  4.4  1.5  0.4]\n",
      "Example:  16  label:  0  features:  [ 5.4  3.9  1.3  0.4]\n",
      "Example:  17  label:  0  features:  [ 5.1  3.5  1.4  0.3]\n",
      "Example:  18  label:  0  features:  [ 5.7  3.8  1.7  0.3]\n",
      "Example:  19  label:  0  features:  [ 5.1  3.8  1.5  0.3]\n",
      "Example:  20  label:  0  features:  [ 5.4  3.4  1.7  0.2]\n",
      "Example:  21  label:  0  features:  [ 5.1  3.7  1.5  0.4]\n",
      "Example:  22  label:  0  features:  [ 4.6  3.6  1.   0.2]\n",
      "Example:  23  label:  0  features:  [ 5.1  3.3  1.7  0.5]\n",
      "Example:  24  label:  0  features:  [ 4.8  3.4  1.9  0.2]\n",
      "Example:  25  label:  0  features:  [ 5.   3.   1.6  0.2]\n",
      "Example:  26  label:  0  features:  [ 5.   3.4  1.6  0.4]\n",
      "Example:  27  label:  0  features:  [ 5.2  3.5  1.5  0.2]\n",
      "Example:  28  label:  0  features:  [ 5.2  3.4  1.4  0.2]\n",
      "Example:  29  label:  0  features:  [ 4.7  3.2  1.6  0.2]\n",
      "Example:  30  label:  0  features:  [ 4.8  3.1  1.6  0.2]\n",
      "Example:  31  label:  0  features:  [ 5.4  3.4  1.5  0.4]\n",
      "Example:  32  label:  0  features:  [ 5.2  4.1  1.5  0.1]\n",
      "Example:  33  label:  0  features:  [ 5.5  4.2  1.4  0.2]\n",
      "Example:  34  label:  0  features:  [ 4.9  3.1  1.5  0.1]\n",
      "Example:  35  label:  0  features:  [ 5.   3.2  1.2  0.2]\n",
      "Example:  36  label:  0  features:  [ 5.5  3.5  1.3  0.2]\n",
      "Example:  37  label:  0  features:  [ 4.9  3.1  1.5  0.1]\n",
      "Example:  38  label:  0  features:  [ 4.4  3.   1.3  0.2]\n",
      "Example:  39  label:  0  features:  [ 5.1  3.4  1.5  0.2]\n",
      "Example:  40  label:  0  features:  [ 5.   3.5  1.3  0.3]\n",
      "Example:  41  label:  0  features:  [ 4.5  2.3  1.3  0.3]\n",
      "Example:  42  label:  0  features:  [ 4.4  3.2  1.3  0.2]\n",
      "Example:  43  label:  0  features:  [ 5.   3.5  1.6  0.6]\n",
      "Example:  44  label:  0  features:  [ 5.1  3.8  1.9  0.4]\n",
      "Example:  45  label:  0  features:  [ 4.8  3.   1.4  0.3]\n",
      "Example:  46  label:  0  features:  [ 5.1  3.8  1.6  0.2]\n",
      "Example:  47  label:  0  features:  [ 4.6  3.2  1.4  0.2]\n",
      "Example:  48  label:  0  features:  [ 5.3  3.7  1.5  0.2]\n",
      "Example:  49  label:  0  features:  [ 5.   3.3  1.4  0.2]\n",
      "Example:  50  label:  1  features:  [ 7.   3.2  4.7  1.4]\n",
      "Example:  51  label:  1  features:  [ 6.4  3.2  4.5  1.5]\n",
      "Example:  52  label:  1  features:  [ 6.9  3.1  4.9  1.5]\n",
      "Example:  53  label:  1  features:  [ 5.5  2.3  4.   1.3]\n",
      "Example:  54  label:  1  features:  [ 6.5  2.8  4.6  1.5]\n",
      "Example:  55  label:  1  features:  [ 5.7  2.8  4.5  1.3]\n",
      "Example:  56  label:  1  features:  [ 6.3  3.3  4.7  1.6]\n",
      "Example:  57  label:  1  features:  [ 4.9  2.4  3.3  1. ]\n",
      "Example:  58  label:  1  features:  [ 6.6  2.9  4.6  1.3]\n",
      "Example:  59  label:  1  features:  [ 5.2  2.7  3.9  1.4]\n",
      "Example:  60  label:  1  features:  [ 5.   2.   3.5  1. ]\n",
      "Example:  61  label:  1  features:  [ 5.9  3.   4.2  1.5]\n",
      "Example:  62  label:  1  features:  [ 6.   2.2  4.   1. ]\n",
      "Example:  63  label:  1  features:  [ 6.1  2.9  4.7  1.4]\n",
      "Example:  64  label:  1  features:  [ 5.6  2.9  3.6  1.3]\n",
      "Example:  65  label:  1  features:  [ 6.7  3.1  4.4  1.4]\n",
      "Example:  66  label:  1  features:  [ 5.6  3.   4.5  1.5]\n",
      "Example:  67  label:  1  features:  [ 5.8  2.7  4.1  1. ]\n",
      "Example:  68  label:  1  features:  [ 6.2  2.2  4.5  1.5]\n",
      "Example:  69  label:  1  features:  [ 5.6  2.5  3.9  1.1]\n",
      "Example:  70  label:  1  features:  [ 5.9  3.2  4.8  1.8]\n",
      "Example:  71  label:  1  features:  [ 6.1  2.8  4.   1.3]\n",
      "Example:  72  label:  1  features:  [ 6.3  2.5  4.9  1.5]\n",
      "Example:  73  label:  1  features:  [ 6.1  2.8  4.7  1.2]\n",
      "Example:  74  label:  1  features:  [ 6.4  2.9  4.3  1.3]\n",
      "Example:  75  label:  1  features:  [ 6.6  3.   4.4  1.4]\n",
      "Example:  76  label:  1  features:  [ 6.8  2.8  4.8  1.4]\n",
      "Example:  77  label:  1  features:  [ 6.7  3.   5.   1.7]\n",
      "Example:  78  label:  1  features:  [ 6.   2.9  4.5  1.5]\n",
      "Example:  79  label:  1  features:  [ 5.7  2.6  3.5  1. ]\n",
      "Example:  80  label:  1  features:  [ 5.5  2.4  3.8  1.1]\n",
      "Example:  81  label:  1  features:  [ 5.5  2.4  3.7  1. ]\n",
      "Example:  82  label:  1  features:  [ 5.8  2.7  3.9  1.2]\n",
      "Example:  83  label:  1  features:  [ 6.   2.7  5.1  1.6]\n",
      "Example:  84  label:  1  features:  [ 5.4  3.   4.5  1.5]\n",
      "Example:  85  label:  1  features:  [ 6.   3.4  4.5  1.6]\n",
      "Example:  86  label:  1  features:  [ 6.7  3.1  4.7  1.5]\n",
      "Example:  87  label:  1  features:  [ 6.3  2.3  4.4  1.3]\n",
      "Example:  88  label:  1  features:  [ 5.6  3.   4.1  1.3]\n",
      "Example:  89  label:  1  features:  [ 5.5  2.5  4.   1.3]\n",
      "Example:  90  label:  1  features:  [ 5.5  2.6  4.4  1.2]\n",
      "Example:  91  label:  1  features:  [ 6.1  3.   4.6  1.4]\n",
      "Example:  92  label:  1  features:  [ 5.8  2.6  4.   1.2]\n",
      "Example:  93  label:  1  features:  [ 5.   2.3  3.3  1. ]\n",
      "Example:  94  label:  1  features:  [ 5.6  2.7  4.2  1.3]\n",
      "Example:  95  label:  1  features:  [ 5.7  3.   4.2  1.2]\n",
      "Example:  96  label:  1  features:  [ 5.7  2.9  4.2  1.3]\n",
      "Example:  97  label:  1  features:  [ 6.2  2.9  4.3  1.3]\n",
      "Example:  98  label:  1  features:  [ 5.1  2.5  3.   1.1]\n",
      "Example:  99  label:  1  features:  [ 5.7  2.8  4.1  1.3]\n",
      "Example:  100  label:  2  features:  [ 6.3  3.3  6.   2.5]\n",
      "Example:  101  label:  2  features:  [ 5.8  2.7  5.1  1.9]\n",
      "Example:  102  label:  2  features:  [ 7.1  3.   5.9  2.1]\n",
      "Example:  103  label:  2  features:  [ 6.3  2.9  5.6  1.8]\n",
      "Example:  104  label:  2  features:  [ 6.5  3.   5.8  2.2]\n",
      "Example:  105  label:  2  features:  [ 7.6  3.   6.6  2.1]\n",
      "Example:  106  label:  2  features:  [ 4.9  2.5  4.5  1.7]\n",
      "Example:  107  label:  2  features:  [ 7.3  2.9  6.3  1.8]\n",
      "Example:  108  label:  2  features:  [ 6.7  2.5  5.8  1.8]\n",
      "Example:  109  label:  2  features:  [ 7.2  3.6  6.1  2.5]\n",
      "Example:  110  label:  2  features:  [ 6.5  3.2  5.1  2. ]\n",
      "Example:  111  label:  2  features:  [ 6.4  2.7  5.3  1.9]\n",
      "Example:  112  label:  2  features:  [ 6.8  3.   5.5  2.1]\n",
      "Example:  113  label:  2  features:  [ 5.7  2.5  5.   2. ]\n",
      "Example:  114  label:  2  features:  [ 5.8  2.8  5.1  2.4]\n",
      "Example:  115  label:  2  features:  [ 6.4  3.2  5.3  2.3]\n",
      "Example:  116  label:  2  features:  [ 6.5  3.   5.5  1.8]\n",
      "Example:  117  label:  2  features:  [ 7.7  3.8  6.7  2.2]\n",
      "Example:  118  label:  2  features:  [ 7.7  2.6  6.9  2.3]\n",
      "Example:  119  label:  2  features:  [ 6.   2.2  5.   1.5]\n",
      "Example:  120  label:  2  features:  [ 6.9  3.2  5.7  2.3]\n",
      "Example:  121  label:  2  features:  [ 5.6  2.8  4.9  2. ]\n",
      "Example:  122  label:  2  features:  [ 7.7  2.8  6.7  2. ]\n",
      "Example:  123  label:  2  features:  [ 6.3  2.7  4.9  1.8]\n",
      "Example:  124  label:  2  features:  [ 6.7  3.3  5.7  2.1]\n",
      "Example:  125  label:  2  features:  [ 7.2  3.2  6.   1.8]\n",
      "Example:  126  label:  2  features:  [ 6.2  2.8  4.8  1.8]\n",
      "Example:  127  label:  2  features:  [ 6.1  3.   4.9  1.8]\n",
      "Example:  128  label:  2  features:  [ 6.4  2.8  5.6  2.1]\n",
      "Example:  129  label:  2  features:  [ 7.2  3.   5.8  1.6]\n",
      "Example:  130  label:  2  features:  [ 7.4  2.8  6.1  1.9]\n",
      "Example:  131  label:  2  features:  [ 7.9  3.8  6.4  2. ]\n",
      "Example:  132  label:  2  features:  [ 6.4  2.8  5.6  2.2]\n",
      "Example:  133  label:  2  features:  [ 6.3  2.8  5.1  1.5]\n",
      "Example:  134  label:  2  features:  [ 6.1  2.6  5.6  1.4]\n",
      "Example:  135  label:  2  features:  [ 7.7  3.   6.1  2.3]\n",
      "Example:  136  label:  2  features:  [ 6.3  3.4  5.6  2.4]\n",
      "Example:  137  label:  2  features:  [ 6.4  3.1  5.5  1.8]\n",
      "Example:  138  label:  2  features:  [ 6.   3.   4.8  1.8]\n",
      "Example:  139  label:  2  features:  [ 6.9  3.1  5.4  2.1]\n",
      "Example:  140  label:  2  features:  [ 6.7  3.1  5.6  2.4]\n",
      "Example:  141  label:  2  features:  [ 6.9  3.1  5.1  2.3]\n",
      "Example:  142  label:  2  features:  [ 5.8  2.7  5.1  1.9]\n",
      "Example:  143  label:  2  features:  [ 6.8  3.2  5.9  2.3]\n",
      "Example:  144  label:  2  features:  [ 6.7  3.3  5.7  2.5]\n",
      "Example:  145  label:  2  features:  [ 6.7  3.   5.2  2.3]\n",
      "Example:  146  label:  2  features:  [ 6.3  2.5  5.   1.9]\n",
      "Example:  147  label:  2  features:  [ 6.5  3.   5.2  2. ]\n",
      "Example:  148  label:  2  features:  [ 6.2  3.4  5.4  2.3]\n",
      "Example:  149  label:  2  features:  [ 5.9  3.   5.1  1.8]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(iris.target)):\n",
    "    print(\"Example: \", i, \" label: \", iris.target[i], \" features: \", iris.data[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know how to work with the data set, we're ready to train a classifier. But before we do that, first we need to split up the data. I'm going to remove several of the examples and put them aside for later. We'll call the examples I'm putting aside our testing data. We'll keep these separate from our training data, and later on we'll use our testing examples to test how accurate the classifier is on data it's never seen before. Testing is actually a really important part of doing machine learning well in practice, and we'll cover it in more detail in a future episode. Just for this exercise, I'll remove one example of each type of flower. And as it happens, the data set is ordered so the first setosa is at index 0, and the first versicolor is at 50, and so on. The syntax looks a little bit complicated, but all I'm doing is removing three entries from the data and target variables. Then I'll create two new sets of variables-- one for training and one for testing. Training will have the majority of our data, and testing will have just the examples I removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, just as before, we can create a decision tree classifier and train it on our training data. Before we visualize it, let's use the tree to classify our testing data. We know we have one flower of each type, and we can print out the labels we expect. Now let's see what the tree predicts. We'll give it the features for our testing data, and we'll get back labels. You can see the predicted labels match our testing data. That means it got them all right. Now, keep in mind, this was a very simple test, and we'll go into more detail down the road."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2]\n",
      "[0 1 2]\n"
     ]
    }
   ],
   "source": [
    "# Train a classifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn import tree\n",
    "import numpy as np\n",
    "\n",
    "iris = load_iris()\n",
    "test_idx = [0, 50, 100]\n",
    "\n",
    "# training data\n",
    "train_target = np.delete(iris.target, test_idx)\n",
    "train_data = np.delete(iris.data, test_idx, axis=0)\n",
    "\n",
    "# testing data\n",
    "test_target = iris.target[test_idx]\n",
    "test_data = iris.data[test_idx]\n",
    "\n",
    "clf_tree = tree.DecisionTreeClassifier()\n",
    "clf_tree.fit(train_data, train_target)\n",
    "\n",
    "# Predict label for new flower\n",
    "print(test_target)\n",
    "print(clf_tree.predict(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's visualize the tree so we can see how the classifier works. To do that, I'm going to copy-paste some code in from scikit's tutorials, and because this code is for visualization and not machine-learning concepts, I won't cover the details here. Note that I'm combining the code from these two examples to create an easy-to-read PDF. I can run our script and open up the PDF, and we can see the tree.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Visualize the tree\n",
    "from sklearn.externals.six import StringIO\n",
    "from IPython.display import Image\n",
    "\n",
    "dot_data = StringIO()\n",
    "tree.export_graphviz(clf_tree,\n",
    "                    out_file=\"tree.dot\",\n",
    "                    feature_names=iris.feature_names,\n",
    "                    class_names=iris.target_names,\n",
    "                    filled=True, rounded=True,\n",
    "                    impurity=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
