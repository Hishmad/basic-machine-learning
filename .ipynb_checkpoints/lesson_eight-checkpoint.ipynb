{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Lesson Eight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### TensorBoard\n",
    "According to the documentations:\n",
    "The computations you'll use TensorFlow for - like training a massive deep neural network - can be complex and confusing. To make it easier to understand, debug, and optimize TensorFlow programs, we've included a suite of visualization tools called TensorBoard. You can use TensorBoard to visualize your TensorFlow graph, plot quantitative metrics about the execution of your graph, and show additional data like images that pass through it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Let's import from lesson_six\n",
    "import lesson_six as lesson_six\n",
    "# Rest the graph\n",
    "lesson_six.reset_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "# Instanciat the data\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "# The shape of the data is (20640, 8) this will return m is the number of rows, n is the number of column\n",
    "m, n = housing.data.shape \n",
    "#housing_data_and_bias = np.c_[np.ones((m, 1)), housing.data]\n",
    "\n",
    "# We have to standardiaze the input data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Instantiate the preprocessing object\n",
    "scaler = StandardScaler() \n",
    "\n",
    "# This will standardize the input data, the shape will be (m, 8)\n",
    "scaled_housing_data = scaler.fit_transform(housing.data) \n",
    "\n",
    "# This will add bias, this will add column at index 0, the shape will be (m, 8+1) \n",
    "scaled_housing_data_and_bias = np.c_[np.ones((m, 1)), scaled_housing_data] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def fetch_batch(epoch, batch_index, batch_size):\n",
    "    '''\n",
    "    This fucntion will return the X_batch, y_batch with random indices\n",
    "    '''\n",
    "    \n",
    "    # Find a random indices whithin the size of the dataset\n",
    "    np.random.seed(epoch * n_batches + batch_index)\n",
    "    indices = np.random.randint(m, size=batch_size)\n",
    "    \n",
    "    # Pich the X and y batches using the random indices\n",
    "    X_batch = scaled_housing_data_and_bias[indices]\n",
    "    y_batch = housing.target.reshape(-1, 1)[indices]\n",
    "    return X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# First we will create a log\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def getLogDir():\n",
    "    now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "    root_logdir = \"/Volumes/MacAndroidStudio/ml-books/savedModel/tf_logs\"\n",
    "    logdir = \"{}/run-{}/\".format(root_logdir, now)\n",
    "    \n",
    "    return logdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Change the definition of X and y in the construction phase to make them a placeholder nodes.\n",
    "X = tf.placeholder(tf.float32, shape=(None, n + 1), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "\n",
    "# Compute the theta with tf.random_uniform() use the number of column add 1 so it will match the X+bias columns\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "\n",
    "# Compute the y_hat using tf.matmul()\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "\n",
    "# Get the error\n",
    "error = y_pred - y\n",
    "\n",
    "# Compute the Mean Square Error using tf.reduce_mean() and tf.square(error)\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "\n",
    "# Gradient Decscent with optimizer\n",
    "#optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.5)\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "# Globa variable initializer\n",
    "global_init = tf.global_variables_initializer()\n",
    "\n",
    "# This will create a node in the graph that will evaluate the MSE value and write it to a TensorBoard\n",
    "# binary log string called a summary.\n",
    "mse_summary = tf.summary.scalar('MSE', mse)\n",
    "\n",
    "# This will create FileWriter object that we will use to write summaries to logfiles in the log directory\n",
    "# The first parameter is the log directory path, and the second parameter which optional is the graph to visualize.\n",
    "file_writer = tf.summary.FileWriter(getLogDir(), tf.get_default_graph())\n",
    "\n",
    "# Define the batch size and compute the total number of batches:\n",
    "batch_size = 100\n",
    "n_batches = int(np.ceil(m / batch_size))\n",
    "\n",
    "# The running phase...\n",
    "with tf.Session() as sess:\n",
    "    # Pass the tf.global_variables_initializer() to the Session.run()\n",
    "    sess.run(global_init)                                                                \n",
    "\n",
    "    # iterate over\n",
    "    for epoch in range(n_epochs): \n",
    "        \n",
    "        for batch_index in range(n_batches):\n",
    "            \n",
    "            # Fetching the mini-batches one by one\n",
    "            X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size)\n",
    "            \n",
    "            # For every 10 mini-batches this code will evaluate the mse_summary node during training.\n",
    "            # Then write the result to the file log using the FileWriter object\n",
    "            if batch_index % 10 == 0:\n",
    "                summary_str = mse_summary.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                step = epoch * n_batches + batch_index\n",
    "                file_writer.add_summary(summary_str, step)\n",
    "            \n",
    "            # Invoke run() and pass the training optimizer, with the X and y batches\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "\n",
    "    # Pass the result here\n",
    "    best_theta = theta.eval() \n",
    "\n",
    "# Close the FileWriter to free resources.\n",
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.049927  ],\n",
       "       [ 0.8267504 ],\n",
       "       [ 0.11433333],\n",
       "       [-0.23890038],\n",
       "       [ 0.31248516],\n",
       "       [ 0.03146332],\n",
       "       [-1.4988426 ],\n",
       "       [-0.87141562],\n",
       "       [-0.83012849]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "Fromt the terminal type this:\n",
    "$ tensorboard --logdir=path/to/log-directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back to basic again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lesson_six.reset_graph()\n",
    "\n",
    "# Create a global variable\n",
    "W = tf.Variable([.3], dtype=tf.float32)\n",
    "b = tf.Variable([-.3], dtype=tf.float32)\n",
    "\n",
    "# Create a placeholder: it is a promise to provide a value later.\n",
    "x = tf.placeholder(tf.float32)\n",
    "\n",
    "# This is a linear operation\n",
    "linear_model = W * x + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This will initialize all the global variables, in this case W and b\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Instantiate the Session()\n",
    "sess = tf.Session()\n",
    "\n",
    "# Pass the init to the run() \n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.          0.30000001  0.60000002  0.90000004]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Here where all the computation is done:\n",
    "# linear_mobel has a reference to W, x and b, in which its already know the value of W and b but not yet x\n",
    "# because x is a placeholder and it is a promise to provide value later\n",
    "# So this will result in processing every element in x, in according to linear_model.\n",
    "print(sess.run(linear_model, {x:[1,2,3,4]}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.66\n"
     ]
    }
   ],
   "source": [
    "# y is the target value, or often called the label\n",
    "y = tf.placeholder(tf.float32)\n",
    "\n",
    "# The tf.square() will square the error\n",
    "squared_deltas = tf.square(linear_model - y)\n",
    "\n",
    "# The tf.reduce_sum() will get the loss of the squared error\n",
    "loss = tf.reduce_sum(squared_deltas)\n",
    "\n",
    "# Now lets evaluate and pass the data for x and y\n",
    "print(sess.run(loss, {x:[1,2,3,4], y:[0,-1,-2,-3]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-1.], dtype=float32), array([ 1.], dtype=float32)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The tf.assign is to change/modify the value of a variable\n",
    "fixW = tf.assign(W, [-1.])\n",
    "fixb = tf.assign(b, [1.])\n",
    "\n",
    "# this is same as initializeing\n",
    "sess.run([fixW, fixb])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# This will evaluate the loss, given the value of x and y\n",
    "print(sess.run(loss, {x:[1,2,3,4], y:[0,-1,-2,-3]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consequently, TensorFlow can automatically produce derivatives given only a description of the model using the function tf.gradients. For simplicity, optimizers typically do this for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We pass the learning rate as argument\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "\n",
    "# then we pass the loss to the optimizer.minimize()\n",
    "train = optimizer.minimize(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess.run(init) # reset values to incorrect defaults.\n",
    "for i in range(1000):\n",
    "    sess.run(train, {x:[1,2,3,4], y:[0,-1,-2,-3]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W: [-0.9999969] b: [ 0.99999082] loss: 5.69997e-11\n"
     ]
    }
   ],
   "source": [
    "# evaluate training accuracy\n",
    "curr_W, curr_b, curr_loss = sess.run([W, b, loss], {x:[1,2,3,4], y:[0,-1,-2,-3]})\n",
    "print(\"W: %s b: %s loss: %s\"%(curr_W, curr_b, curr_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From the documentations: \n",
    "\n",
    "tf.contrib.learn is a high-level TensorFlow library that simplifies the mechanics of machine learning, including the following:\n",
    "\n",
    "running training loops\n",
    "running evaluation loops\n",
    "managing data sets\n",
    "managing feeding\n",
    "tf.contrib.learn defines many common models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
