{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Lesson Five"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "In this episode, we're going to do something special, and that's write our own classifier from scratch. If you're new to machine learning, this is a big milestone. Because if you can follow along and do this on your own, it means you understand an important piece of the puzzle. The classifier we're going to write today is a scrappy version of k-Nearest Neighbors. That's one of the simplest classifiers around. First, here's a quick outline of what we'll do in this episode. We'll start with our code from Episode 4, Letâ€™s Write a Pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.973333333333\n"
     ]
    }
   ],
   "source": [
    "# Recall that lesson we did a simple experiment.\n",
    "# We imported the datasets\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# X is the feature (input)\n",
    "X = iris.data\n",
    "\n",
    "# y is the label (output)\n",
    "y = iris.target\n",
    "\n",
    "# We split out datasets into train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5)\n",
    "\n",
    "# WE import KNeighborsClassifier \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "my_classifier = KNeighborsClassifier()\n",
    "\n",
    "# we used train to train a classifier\n",
    "my_classifier.fit(X_train, y_train)\n",
    "\n",
    "# We tested to see how accurate it was\n",
    "predictions = my_classifier.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Writing the classifier is the part we're going to focus on today. Previously we imported the classifier from a library using these two lines. Here we'll comment them out and write our own. The rest of the pipeline will stay exactly the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.346666666667\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Step 1: comment out imports, and write out own, the rest of the pipeline will stay exactly the same\n",
    "#from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Step 2: implemet a class for our custom classifier\n",
    "class ScrappyKKN():\n",
    "    # Step 3: Let see what methods we need to implement. Looking a the interface for classifier,\n",
    "    # we see there are two that we care about, fit() which does the training, and predict() which does\n",
    "    # the prediction.\n",
    "    \n",
    "    def fit(self, X_train, y_train):\n",
    "        '''\n",
    "        This fit() method will do the training\n",
    "        Remember it takes the features and labels for the training set as input parameters.\n",
    "        '''\n",
    "        # Lets store the training data in this class, you can think of this just memorizing it.\n",
    "        # And you will see why we do that later on.\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "            \n",
    "        pass\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        '''\n",
    "        As parameters, this receives the features for out testing data.\n",
    "        And as output it returns predictions for the labels\n",
    "        '''\n",
    "        \n",
    "        # Remember that we'll need to return a list of predictions. That;s because the parameter, X_test is\n",
    "        # actually a 2D array, or list of lists.\n",
    "        predictions = []\n",
    "        \n",
    "        # each row contains the features for one testing example.\n",
    "        for row in X_test:\n",
    "            # To make a prediction for each row, I will just randomly pick a label from the training data.\n",
    "            # and append that to our predictions.\n",
    "            label = random.choice(self.y_train)\n",
    "            predictions.append(label)\n",
    "        \n",
    "        return predictions\n",
    "\n",
    "# Change our pipline to use the customer class\n",
    "# At this point our pipline is working again\n",
    "my_classifier = ScrappyKKN()\n",
    "\n",
    "# we used train to train a classifier\n",
    "my_classifier.fit(X_train, y_train)\n",
    "\n",
    "# We tested to see how accurate it was\n",
    "predictions = my_classifier.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Recall there are three different types of flowers in the iris dataset, so accuracy should be about 33%. Now we know the interface for a classifier. But when we started this exercise, our accuracy was above 90%. So let's see if we can do better. To do that, we'll implement our classifier, which is based on k-Nearest Neighbors. Here's the intuition for how that algorithm works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now we know the interface for a classifier. But when we started this exercise, our accuracy was above 90%. So let's see if we can do better. To do that, we'll implement our classifier, which is based on k-Nearest Neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "There's a formula for that called the Euclidean Distance, and here's what the formula looks like. It measures the distance between two points, and it works a bit like the Pythagorean Theorem. A squared plus B squared equals C squared. You can think of this term as A, or the difference between the first two features. Likewise, you can think of this term as B, or the difference between the second pair of features. And the distance we compute is the length of the hypotenuse. Now here's something cool. Right now we're computing distance in two-dimensional space, because we have just two features in our toy dataset. But what if we had three features or three dimensions? Well then we'd be in a cube. We can still visualize how to measure distance in the space with a ruler. But what if we had four features or four dimensions, like we do in iris? Well, now we're in a hypercube, and we can't visualize this very easy. The good news is the Euclidean Distance works the same way regardless of the number of dimensions. With more features, we can just add more terms to the equation. You can find more details about this online. Now let's code up Euclidean distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.96\n"
     ]
    }
   ],
   "source": [
    "# Step 7: implement nearest neighbor algorithm for a classifier.\n",
    "# To make a prediction for test point we'll calculate the distance to all the training points.\n",
    "# Then we'll predict the testing point has the same label as the closest one.\n",
    "\n",
    "# Let's code up Eculidean distance, there are plenty of ways to do that, but \n",
    "# we'll use a library called scipy\n",
    "from scipy.spatial import distance\n",
    "\n",
    "class ScrappyKKN():\n",
    "    '''\n",
    "    Complete version\n",
    "    '''\n",
    "    \n",
    "    def fit(self, X_train, y_train):\n",
    "        '''\n",
    "        This fit() method will do the training\n",
    "        Remember it takes the features and labels for the training set as input parameters.\n",
    "        '''\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "            \n",
    "        pass\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        '''\n",
    "        As parameters, this receives the features for out testing data.\n",
    "        And as output it returns predictions for the labels\n",
    "        '''\n",
    "        \n",
    "        predictions = []\n",
    "        for row in X_test:\n",
    "            \n",
    "            # delete the rancom prediction wen made\n",
    "            #label = random.choice(self.y_train)\n",
    "            # replcae it with the a method that finds the closest training\n",
    "            # to the test point\n",
    "            label = self.closest(row)\n",
    "            \n",
    "            predictions.append(label)\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    def closest(self, row):\n",
    "        '''\n",
    "        In this method we will loop over all the training points, and keep track of the \n",
    "        closest one so far. Remember that we memorized the training data in out self.fit()\n",
    "        '''\n",
    "        \n",
    "        # To start I will calculate the distance from the test point to the first training point.\n",
    "        best_dist = euc(row, self.X_train[0])\n",
    "        # I will use this variable to keep track of the index of the training point that's closest.\n",
    "        # We'll need this later to retrieve its label\n",
    "        best_index = 0\n",
    "        \n",
    "        # Now we'll iterate over all the other taining points.\n",
    "        for i in range(1, len(self.X_train)):\n",
    "            dist = euc(row, self.X_train[i])\n",
    "            \n",
    "            # And every time we find a closer one, we will update out variables.\n",
    "            if dist < best_dist:\n",
    "                best_dist = dist\n",
    "                best_index = i\n",
    "        \n",
    "        # Finally we will use the index to return the label for the closst training example.\n",
    "        return self.y_train[best_index]\n",
    "        \n",
    "    def euc(a, b):\n",
    "        '''\n",
    "        Here (a) and (b) are lists of numeric features. Say (a) is a point from out training data,\n",
    "        and (b) is a point from our testing data. This function returns the distance between them.\n",
    "        '''\n",
    "        return distance.euclidean(a, b)\n",
    "\n",
    "\n",
    "\n",
    "my_classifier = ScrappyKKN()\n",
    "my_classifier.fit(X_train, y_train)\n",
    "predictions = my_classifier.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We did get accuracy over 90%.... awesome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now if you can code this up and understand it, that's a big accomplishment because it means you can write a simple classifier from scratch. Now, there are a number of pros and cons to this algorithm, many of which you can find online. The basic pro is that it's relatively easy to understand, and works reasonably well for some problems. And the basic cons are that it's slow, because it has to iterate over every training point to make a prediction. And importantly, as we saw in Episode 3, some features are more informative than others. But there's not an easy way to represent that in k-Nearest Neighbors. In the long run, we want a classifier that learns more complex relationships between features and the label we're trying to predict. A decision tree is a good example of that. And a neural network like we saw in TensorFlow Playground is even better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
